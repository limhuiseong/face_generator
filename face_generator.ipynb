{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3BzJKSw0CAZ",
        "outputId": "8cd1261c-994c-4ad0-d592-5f047b4a2337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j44bncCtVrn5",
        "outputId": "5fefc61c-f676-4894-c1f8-399302171ede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version : 2.9.2\n",
            "Python version : 3.8.16 (default, Dec  7 2022, 01:12:13) \n",
            "[GCC 7.5.0]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "print(\"TensorFlow version : \" + tf.__version__)\n",
        "print(\"Python version : \" + sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDHgHVAd0NTS",
        "outputId": "1755dc75-1c58-435e-accb-39230a8badc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading celeba-dataset.zip to /content\n",
            " 98% 1.30G/1.33G [00:15<00:00, 243MB/s]\n",
            "100% 1.33G/1.33G [00:15<00:00, 91.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"drive/MyDrive/kaggle/\"\n",
        "\n",
        "!kaggle datasets download -d jessicali9530/celeba-dataset\n",
        "!unzip -q celeba-dataset.zip -d ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmYRyGcC0pzh"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "file_list = os.listdir('/content/img_align_celeba/img_align_celeba')\n",
        "images = []\n",
        "\n",
        "for i in file_list[0:50000]:\n",
        "  images_to_number = Image.open('/content/img_align_celeba/img_align_celeba/' + i).crop((30,40,160,170)).convert('L').resize(( 64,64))\n",
        "  images.append(np.array(images_to_number))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(images[1])\n",
        "plt.show()\n",
        "\n",
        "images = np.array(images)\n",
        "print(images.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0pFzmnK3_mq",
        "outputId": "542506ba-f91d-4755-fd83-f5c23da8b3e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 64, 64, 1)\n"
          ]
        }
      ],
      "source": [
        "images = np.divide(images,255)\n",
        "images = images.reshape(50000,64,64,1)\n",
        "print(images.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KkeSwgVk8bG"
      },
      "outputs": [],
      "source": [
        "discriminator = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), strides = (2, 2), padding = 'same', input_shape=[64, 64, 1]),\n",
        "    tf.keras.layers.LeakyReLU(alpha = 0.2),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), strides = (2, 2), padding = 'same'),\n",
        "    tf.keras.layers.LeakyReLU(alpha = 0.2),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), strides = (2, 2), padding = 'same'),\n",
        "    tf.keras.layers.LeakyReLU(alpha = 0.2),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), strides = (2, 2), padding = 'same'),\n",
        "    tf.keras.layers.LeakyReLU(alpha = 0.2),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6UrQ4T6mybw"
      },
      "outputs": [],
      "source": [
        "generator = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(4 * 4 * 256, input_shape = (100,)),\n",
        "  tf.keras.layers.Reshape((4, 4, 256)),\n",
        "  tf.keras.layers.Conv2DTranspose(256, 3, strides=2, padding='same'),\n",
        "  tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Conv2DTranspose(128, 3, strides=2, padding='same'),\n",
        "  tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Conv2DTranspose(64, 3, strides=2, padding='same'),\n",
        "  tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Conv2DTranspose(1, 3, strides=2, padding='same', activation='tanh')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BK7ztf4mnU-e"
      },
      "outputs": [],
      "source": [
        "GAN = tf.keras.models.Sequential([generator, discriminator])\n",
        "\n",
        "discriminator.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'binary_crossentropy'\n",
        ")\n",
        "discriminator.trainable = False\n",
        "\n",
        "GAN.compile(optimizer = 'adam', loss = 'binary_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCW2uLJ4tB2e"
      },
      "outputs": [],
      "source": [
        "def picture():\n",
        "  random_number = np.random.uniform(-1, 1, size = (10, 100))\n",
        "  predict = generator.predict(random_number)\n",
        "\n",
        "  for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(predict[i].reshape(64, 64), cmap = 'gray')\n",
        "    plt.axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLGaRkghCyYo"
      },
      "outputs": [],
      "source": [
        "for j in range(300):\n",
        "  print(f'{j} epoch 입니다.')\n",
        "  picture()\n",
        "\n",
        "  for i in range(50000//128):\n",
        "    \n",
        "    #Discriminator\n",
        "    real_image = images[i * 128 : (i + 1) * 128]\n",
        "    matrix_filled_with_ones = np.ones(shape = (128, 1))\n",
        "    loss1 = discriminator.train_on_batch(real_image, matrix_filled_with_ones)\n",
        "\n",
        "    random_number = np.random.uniform(-1, 1, size = (128, 100))\n",
        "    face_images = generator.predict(random_number)\n",
        "    matrix_filled_with_zeros = np.zeros(shape = (128, 1))\n",
        "    loss2 = discriminator.train_on_batch(face_images, matrix_filled_with_zeros)\n",
        "\n",
        "    #Generator\n",
        "\n",
        "    random_number = np.random.uniform(-1, 1, size = (128, 100))\n",
        "    matrix_filled_with_ones = np.ones(shape = (128,1))\n",
        "\n",
        "    loss3 = GAN.train_on_batch(random_number, matrix_filled_with_ones)\n",
        "\n",
        "  print(f'Discriminator의 loss는 {loss1 + loss2}입니다.\\nGenerator의 loss는 {loss3}입니다.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GAN.save('face_generator.h5')"
      ],
      "metadata": {
        "id": "x1vKKtqnqLrA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}